{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Problem Statement\n \n\n### Business Problem Overview\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business goal. \n\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn. \n\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n\n### Understanding and Defining Churn:\n\n1. There are two main models of payment in the telecom industry - postpaid (customers pay a monthly/annual bill after using the services) and prepaid (customers pay/recharge with a certain amount in advance and then use the services).\n\n\n2. In the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\n\n\n3. However, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\n\n4. Thus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‘churn’ should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\n\nThis project is based on the Indian and Southeast Asian market.\n\nDefinitions of Churn:\n\nThere are various ways to define churn, such as:\n\n1. Revenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‘customers who have generated less than INR 4 per month in total/average/median revenue’.\n\n2. The main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\n\n3. Usage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\n\n4. A potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n\nIn this project, you will use the usage-based definition to define churn.\n\n1. High-value Churn:\n\n1. In the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n\n2. In this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\n\n \n\n### Understanding the Business Objective and the Data:\n\n1. The dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. \n\n\n2. The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n\n\n### Understanding Customer Behaviour During Churn:\n\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n\n1. The ‘good’ phase: \nIn this phase, the customer is happy with the service and behaves as usual.\n\n2. The ‘action’ phase: \nThe customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n\n3. The ‘churn’ phase: \nIn this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.\n\n \nIn this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase.\n\n### Data Dictionary:\n\nThe dataset can be download using this link. The data dictionary is provided for download below.\n\n1. Data Dictionary - Telecom Churnfile_download\tDownload\nThe data dictionary contains meanings of abbreviations. Some frequent ones are loc (local), IC (incoming), OG (outgoing), T2T (telecom operator to telecom operator), T2O (telecom operator to another operator), RECH (recharge) etc.\n\n \n2. The attributes containing 6, 7, 8, 9 as suffixes imply that those correspond to the months 6, 7, 8, 9 respectively.\n\n### Data Preparation:\n\nThe following data preparation steps are crucial for this problem:\n\n\n1. Derive new features\n\nThis is one of the most important parts of data preparation since good features are often the differentiators between good and bad models. Use your business understanding to derive features you think could be important indicators of churn.\n\n \n2. Filter high-value customers\n\nAs mentioned above, you need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\n\n After filtering the high-value customers, you should get about 29.9k rows.\n\n3. Tag churners and remove attributes of the churn phase\n\nNow tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n\ntotal_ic_mou_9\n\ntotal_og_mou_9\n\nvol_2g_mb_9\n\nvol_3g_mb_9\n\n\nAfter tagging churners, remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names).\n\n### Modelling:\n\nBuild models to predict churn. The predictive model that you’re going to build will serve two purposes:\n\nIt will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\n\nIt will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks.\n\nIn some cases, both of the above-stated goals can be achieved by a single machine learning model. But here, you have a large number of attributes, and thus you should try using a dimensionality reduction technique such as PCA and then build a predictive model. After PCA, you can use any classification model.\n\nAlso, since the rate of churn is typically low (about 5-10%, this is called class-imbalance) - try using techniques to handle class imbalance. \n\n\nYou can take the following suggestive steps to build the model:\n\n1. Preprocess data (convert columns to appropriate formats, handle missing values, etc.)\n\nConduct appropriate exploratory analysis to extract useful insights (whether directly useful for business or for eventual modelling/feature engineering).\n\n2. Derive new features.\n\n3. Reduce the number of variables using PCA.\n\n4. Train a variety of models, tune model hyperparameters, etc. (handle class imbalance using appropriate techniques).\n\n5. Evaluate the models using appropriate evaluation metrics. Note that is is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.\n\n6. Finally, choose a model based on some evaluation metric.\n\n7. The above model will only be able to achieve one of the two goals - to predict customers who will churn. You can’t use the above model to identify the important features for churn. That’s because PCA usually creates components which are not easy to interpret.\n\n8. Therefore, build another model with the main objective of identifying important predictor attributes which help the business understand indicators of churn. A good choice to identify important variables is a logistic regression model or a model from the tree family. In case of logistic regression, make sure to handle multi-collinearity.\n\n9. After identifying important predictors, display them visually - you can use plots, summary tables etc. - whatever you think best conveys the importance of features. \n\n10. Finally, recommend strategies to manage customer churn based on your observations.\n \n\nNote: Everything has to be submitted in one Jupyter notebook.\n\n \n\nThe evaluation rubrics are mentioned on the next page."},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Reading & Understanding:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import the Necessary Libraries\n\nimport numpy as np\nimport pandas as pd\n\n# Import Visualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import the logistic regression \nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Import the scaler, KMeans etc.,\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree\nimport os\n\n# Supress the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read the Csv file & Read the few coloumns and rows of the file. \n\nChurn = pd.read_csv('Downloads//telecom_churn_data.csv')\nChurn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the data shape\nChurn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We have 99999 rows and 226 coloumns. \n# Let us check then datatype \nChurn.info(verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the statistics \nChurn.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Cleaning:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us see the NAN values % in ascending order wise\nround(Churn.isnull().mean(axis=0).sort_values(ascending=False)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lot of values is having more than 70% of the data null. We need to check the unique values first and deal it one by one.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the unique values in sorting order and check the data / value first.\nChurn.nunique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Missing values percentage of all the coloumns in sorting fashion\nround(Churn.isnull().mean(axis=0).sort_values(ascending=False)*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Imputation :"},{"metadata":{"trusted":false},"cell_type":"code","source":"# As We see more than 40% of varaibles are needed for solving the business problem. We need to imute it rather than drop it.\n# Let us make a custom function for the data cleaning part.\n# Create a funciton and check the % values and print the missing nos. \ndef checknull(per_cutoff):\n    missing = round(100*(Churn.isnull().sum()/Churn.shape[0]))\n    print(\"There are {} features having more than {}% missing value\".format(len(missing.loc[missing > per_cutoff]),per_cutoff))\n    return missing.loc[missing > per_cutoff]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us also create the Imute functions so that we can imute it one by one. \ndef imputenan(data,imputeColList=False,missingColList=False):\n    # Function impute the nan with 0\n    # argument: colList, list of columns for which nan is to be replaced with 0\n    if imputeColList:\n        for column in [x + y for y in ['_6','_7','_8','_9'] for x in imputeColList]:\n            data[column].fillna(0, inplace=True)\n    else:    \n        for column in missingColList:\n            data[column].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the missing values again\nchecknull(70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see the KPI's are also there in the above sets. we need to imute it rather than drop it. Let us impute it with the zero.\nimputeCol = ['av_rech_amt_data', 'arpu_2g', 'arpu_3g', 'count_rech_2g', 'count_rech_3g',\n             'max_rech_data', 'total_rech_data','fb_user','night_pck_user']\nimputenan(Churn,imputeCol)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the data again now.\nchecknull(70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# dropping the columns having more than 50% missing values\nmissingcol = list(checknull(70).index)\nChurn.drop(missingcol,axis=1,inplace=True)\nChurn.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the head again\nChurn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# the Circle id is also of no use having a unique value in this \nChrun = Churn.drop('circle_id',axis=1,inplace = True)\nChurn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check again the missing values back in the data sets.\nchecknull(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see still there are 29 features having more than missing values. let us check it first.\nmissingcol = list(checknull(5).index)\nprint (\"There are %d customers missing values for %s\"%(len(Churn[Churn[missingcol].isnull().all(axis=1)]),missingcol))\nChurn[Churn[missingcol].isnull().all(axis=1)][missingcol].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us immute it with zero of these customers as we have huge nos. 7745. \nimputenan(Churn,missingColList=missingcol)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the missing null. \nChurn=Churn[~Churn[missingcol].isnull().all(axis=1)]\nChurn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the data set again with more than 2% nan values\nchecknull(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# WE could see still there are 89 features are there. Let us check it first \nmissingcol = list(checknull(2).index)\nprint (\"There are %d customers missing values for %s\"%(len(Churn[Churn[missingcol].isnull().all(axis=1)]),missingcol))\nChurn[Churn[missingcol].isnull().all(axis=1)][missingcol].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the missing null and shape again.\nChurn=Churn[~Churn[missingcol].isnull().all(axis=1)]\nChurn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# There are 381 customers having 89 features. we can imute it with zero.\nmissingcol.remove('date_of_last_rech_8')\nmissingcol.remove('date_of_last_rech_9')\nimputenan(Churn,missingColList=missingcol)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the missing nan again\n# Let us check the data set again\nchecknull(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us create the new data frame and store these features and run the uniqueness to impute it further .\ncolumns = ['loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou','last_date_of_month_8','last_date_of_month_9','date_of_last_rech_6','date_of_last_rech_7', 'date_of_last_rech_8', 'date_of_last_rech_9']\nfor x in columns: \n    print(\"Unique values in column %s are %s\" % (x,Churn[x].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# It seems are single value only. we can impute it with the same.Let us impute with mode first and proceed\ncolumns = ['loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou','last_date_of_month_7','last_date_of_month_8','last_date_of_month_9']\nfor x in columns:\n    print(Churn[x].value_counts())\n    Churn[x].fillna(Churn[x].mode()[0], inplace=True)\nprint(\"All the above features take only one value. Lets impute the missing values in these features with the mode\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We have impute the variables \nchecknull(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# All these features are missing together\nmissingcol = list(checknull(0).index)\nprint (\"There are %d rows in total having missing values for these variables.\"%(len(Churn[Churn[missingcol].isnull().all(axis=1)])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us impute it with the dates frequently coming. \nChurn[Churn['date_of_last_rech_6'].isnull()]['date_of_last_rech_6'] = '6/30/2014'\nChurn[Churn['date_of_last_rech_7'].isnull()]['date_of_last_rech_7'] = '7/31/2014'\nChurn[Churn['date_of_last_rech_8'].isnull()]['date_of_last_rech_8'] = '8/31/2014'\nChurn[Churn['date_of_last_rech_9'].isnull()]['date_of_last_rech_9'] = '9/30/2014'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now we can see that the data is completely cleaned. Let us check the head.\nChurn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We have lot of zeros in the coloumns as a single value. let us drop it and proceed.\n\nSingle_value =Churn.columns[(Churn == 0).all()]\nprint (\"There are {} features only having zero as values. These features are \\n{}\".format(len(Single_value),Single_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can drop these features. \nChurn.drop(Single_value,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Percentage of data left after removing the missing values.\nprint(\"Percentage of data remaining after treating missing values: {}%\".format(round(Churn.shape[0]/99999 *100,2)))\nprint (\"Number of customers: {}\".format(Churn.shape[0]))\nprint (\"Number of features: {}\".format(Churn.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Churn.reset_index(inplace=True,drop=True)\n# list of all columns which store date\ndate_columns = list(Churn.filter(regex='date').columns)\ndate_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Converting dtype of date columns to datetime\nfor col in date_columns:\n    Churn[col] = pd.to_datetime(Churn[col], format='%m/%d/%Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Percentage of data left after removing the missing values.\nChurn.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Derive New Features:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us create the new features. Also Filter high-value customers\n# Defining high-value customers as follows:\n#Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\nrech_col = Churn.filter(regex=('count')).columns\nChurn[rech_col].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# Creating new feature: avg_rech_amt_6,avg_rech_amt_7,avg_rech_amt_8,avg_rech_amt_9\nfor i in range(6,10):\n    Churn['avg_rech_amt_'+str(i)] = round(Churn['total_rech_amt_'+str(i)]/Churn['total_rech_num_'+str(i)]+1,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# impute the NAN values\n\nimputenan(Churn,missingColList=['avg_rech_amt_6','avg_rech_amt_7','avg_rech_amt_8','avg_rech_amt_9'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us create the total recharge amounts of all the months and store it.\n# total recharge amount = count of recharge 2g + count of recharge of 3g of all months and convert to integer data.\nfor i in range(6,10):\n    Churn['total_rech_num_data_'+str(i)] = (Churn['count_rech_2g_'+str(i)]+Churn['count_rech_3g_'+str(i)]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# let us store the total recharge amount data = total rechage number data * average rechage amount data\nfor i in range(6,10):\n    Churn['total_rech_amt_data_'+str(i)] = Churn['total_rech_num_data_'+str(i)] * Churn['av_rech_amt_data_'+str(i)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Another new feature : total month recharge = total recharge amount + total recharge data for each customer each month\nfor i in range(6,10):\n    Churn['total_month_rech_'+str(i)] = Churn['total_rech_amt_'+str(i)]+Churn['total_rech_amt_data_'+str(i)]\nChurn.filter(regex=('total_month_rech')).head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# calculating the avegare of first two months (good phase) total monthly recharge amount\nGood_Phase_avg =(Churn.total_month_rech_6 + Churn.total_month_rech_7)/2\n# finding the cutoff which is the 70th percentile of the good phase average recharge amounts\nCut_off = np.percentile(Good_Phase_avg,70)\n# Filtering the users whose good phase avg. recharge amount >= to the cutoff of 70th percentile.\nHighvalu_users = Churn[Good_Phase_avg >= Cut_off]\n# Reset the index.\nHighvalu_users.reset_index(inplace=True,drop=True)\n\nprint(\"Number of High-Value Customers in the Dataset: %d\\n\"% len(Highvalu_users ))\nprint(\"Percentage High-value users in data : {}%\".format(round(len(Highvalu_users )/Churn.shape[0]*100),2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## We need the tag the churners. \n#Tagging Churners. Now tag the churned customers (churn=1, else 0) based on the fourth month as follows:\n\n#Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes we need to use to tag churners are:\n\n#total_ic_mou_9\n#total_og_mou_9\n#vol_2g_mb_9\n#vol_3g_mb_9\n\ndef getChurnStatus(data,Churn_Phase_Month=9):\n    # Function to tag customers as churners (churn=1, else 0) based on 'vol_2g_mb_','vol_3g_mb_','total_ic_mou_','total_og_mou_'\n    #argument: churnPhaseMonth, indicating the month number to be used to define churn (default= 9)\n    Churn_features= ['vol_2g_mb_','vol_3g_mb_','total_ic_mou_','total_og_mou_']\n    flag = ~data[[s + str(Churn_Phase_Month) for s in Churn_features ]].any(axis=1)\n    flag = flag.map({True:1, False:0})\n    return flag","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the Churn and not churn data for high value customers.\nHighvalu_users['Churn'] = getChurnStatus(Highvalu_users,9)\nprint(\" We have {} users tagged as churners out of {} High Value Customers.\".format(len(Highvalu_users[Highvalu_users.Churn == 1]),Highvalu_users.shape[0]))\nprint(\"High-value Churn Percentage : {}%\".format(round(len(Highvalu_users[Highvalu_users.Churn == 1])/Highvalu_users.shape[0] *100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We could see that this data is highly imbalced in nature and we can see that 8.09% are churn value percentage. "},{"metadata":{"trusted":false},"cell_type":"code","source":"Highvalu_users.head()\nHighvalu_users.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# As we see the REvenue is one of the important parameter. Let us create the total revenues. \n#To find the Average Revenue per unit ARU  = total Revenue / Average Subscribers\n# To get the Total Revenue = Average Revenue per unit ARU * Average Subscribers(5000 nos. of the total)\n\n# Another new feature : total month recharge = total recharge amount + total recharge data for each customer each month\nfor i in range(6,10):\n    Highvalu_users['Total_revenue_'+str(i)] = Highvalu_users['arpu_'+str(i)] * 5000\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Highvalu_users.filter(regex=('Total_revenue_')).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Total Revenue in the Phase wise manner good Phase - first two months, action phase is third month and fourth month is churn phase.\n# Let us see the revenue in the Phase wise manner\n\nHighvalu_users['Good_Phase'] = Highvalu_users['Total_revenue_6'] + Highvalu_users['Total_revenue_7']\nHighvalu_users['Action_Phase'] =  Highvalu_users['Total_revenue_8']\nHighvalu_users['Chrun_Phase'] = Highvalu_users['Total_revenue_9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#After tagging churners, remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names).\nChrun_Phase = Highvalu_users.filter(regex='_9', axis=1)\nHighvalu_users.drop(Chrun_Phase,axis=1,inplace=True)\nHighvalu_users.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Data Analysis\n### Univariate and Bi-Variate Analysis:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's see the correlation matrix \nplt.figure(figsize = (20,20))        # Size of the figure\nsns.heatmap(Highvalu_users.corr(),annot = True, fmt = \".2f\", cmap = \"GnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences:\n\nIt is very difficult to see the correlation of the variables. Let us check for the 80% correlated variables and map again"},{"metadata":{"trusted":false},"cell_type":"code","source":"# correlation matrix\ncorr_matrix = Highvalu_users.corr().abs()\n\n# Selecting the upper triangle of the correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# feature columns with correlation greater than 0.80\nhigh_corr_feat = [column for column in upper.columns if any(upper[column] > 0.80)]\n\nprint(\"HIGHLY CORRELATED FEATURES IN DATA SET:{}\\n\\n{}\".format(len(high_corr_feat), high_corr_feat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Inferences : Around 66 Variables are highly correlated each other more than 80% ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check only these variables correlation and find the inferences and proceed for the univarate and bi-variate analysis.\nnew_corrs = Highvalu_users[['onnet_mou_8', 'loc_og_t2t_mou_8', 'loc_og_t2m_mou_8', 'loc_og_t2f_mou_7', 'loc_og_mou_6', 'loc_og_mou_7', 'loc_og_mou_8', 'std_og_t2t_mou_6', 'std_og_t2t_mou_7', 'std_og_t2t_mou_8', 'std_og_t2m_mou_6', 'std_og_t2m_mou_7', 'std_og_t2m_mou_8', 'isd_og_mou_7', 'isd_og_mou_8', 'total_og_mou_6', 'total_og_mou_7', 'total_og_mou_8', 'loc_ic_t2t_mou_7', 'loc_ic_t2t_mou_8', 'loc_ic_t2m_mou_8', 'loc_ic_mou_6', 'loc_ic_mou_7', 'loc_ic_mou_8', 'std_ic_mou_6', 'std_ic_mou_7', 'std_ic_mou_8', 'total_ic_mou_6', 'total_ic_mou_7', 'total_ic_mou_8', 'total_rech_amt_6', 'total_rech_amt_7', 'total_rech_amt_8', 'count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'av_rech_amt_data_8', 'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8', 'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8', 'sachet_2g_6', 'sachet_2g_7', 'sachet_2g_8', 'monthly_3g_6', 'monthly_3g_7', 'monthly_3g_8', 'sachet_3g_6', 'sachet_3g_7', 'sachet_3g_8', 'avg_rech_amt_7', 'avg_rech_amt_8', 'total_rech_num_data_6', 'total_rech_num_data_7', 'total_rech_num_data_8', 'total_month_rech_6', 'total_month_rech_7', 'total_month_rech_8', 'Total_revenue_6', 'Total_revenue_7', 'Total_revenue_8', 'Good_Phase', 'Action_Phase', 'Chrun_Phase']]\n# Let's see the correlation matrix \nplt.figure(figsize = (20,20))        # Size of the figure\nsns.heatmap(new_corrs.corr(),annot = True, fmt = \".2f\", cmap = \"GnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences :\nThe Total recharge amount for june_6 is postive correlation with the sachet data 2g for June month is 0.9.\n\nThe total revenue for all the months of june, july and aug are highly postive of 0.96 with the total recharge amount\n\nThe Good phase is highly corrlated with the total rechage as 0.8 and action phase as 0.6 and chrun phase 0.5 with the total recharge amount\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the churn in a graphical way \n\nC_IT = sns.catplot(\"Churn\", data = Highvalu_users, aspect=1.5, kind=\"count\", color=\"b\")\nC_IT.set_xticklabels(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences:\nThe Chrun for the highvalue customers is only 8.09%."},{"metadata":{"trusted":false},"cell_type":"code","source":"Highvalu_users.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#check the list of all the columns. \nlist(Highvalu_users.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# the date coloumns are not needed as we have only unique values as well as the mobile nos. Let us drop and proceed\nHighvalu_users.drop(['mobile_number','last_date_of_month_6','last_date_of_month_7','last_date_of_month_8','date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sns.boxplot(y='arpu_6', data=tel)\ncont_cols = [col for col in Highvalu_users.columns if col not in ['Churn']]\nfor col in cont_cols:\n    plt.figure(figsize=(5, 5))\n    sns.boxplot(y=col, data= Highvalu_users)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences : \n We could see that lot of ouliers are there in the data sets. \nThe variables such as Good Phase, Action phase, churn phase, the total Revenue variable of all the months, and all other variabless we can see normaly distributed with outliers.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the Chrun as the target with the other variables \ncont_cols = [col for col in Highvalu_users.columns if col not in ['Churn']]\nfor col in cont_cols:\n    plt.figure(figsize=(5, 5))\n    sns.barplot(x='Churn', y=col, data=Highvalu_users)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences :\n\nThe Good phase we can see the chrun 1 is more compared to action phase.\n\nThe recharge days for the month of _6 that i.e., june churn is more compared to other recharge days. \n\nBut the total revenue for the month of june is higher compared to other phases. \n\nThe Total Amount recharge is lower in the Chrun phase compared to other phases of June, July and Aug months.\n\nThe roaming for the churn case for the month of june is high compared to other months.\n\nThe data 3g is less in june compared to other months. \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the Age of the Network used by the operator Vs the other variables expect churn.\ncont_cols = [col for col in Highvalu_users.columns if col not in ['Churn']]\nfor col in cont_cols:\n    plt.figure(figsize=(5, 5))\n    sns.jointplot(x='aon',y = col,data= Highvalu_users)\n    plt.show()                                                ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inferences : \n    \nWe can see the AON - Age of network is postive correlated to onnet/offnet- minutes usages,roaming incoming/outgoing calls.\n    \nAlso with the 2g, Operator T to other operator mobile network, Operator T to other operator fixed lines & own call centers, std calls.\n\nAlso we can see AON having no correlation with outgoing call others"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the Age of the Network used by the operator Vs the other variables expect churn.\n# rug = True\n# plotting only a few points since rug takes a long while\nsns.distplot(Highvalu_users['aon'][:200], rug=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### Inferences : The data is normally distributed with outliers with respect to age on network.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Model Building:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do the X train , Y train test and seperate the data sets. \n# import needed model building sklearn parameters.\nfrom sklearn.model_selection import train_test_split\nimport sklearn.preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n#putting features variables in X\nX = Highvalu_users.drop(['Churn'], axis=1)\n\n#putting response variables in Y\ny = Highvalu_users['Churn']    \n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"### Scaling before PCA ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Rescaling the features before PCA as it is sensitive to the scales of the features\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fitting and transforming the scaler on train\nX_train = scaler.fit_transform(X_train)\n# transforming the train using the already fit scaler\nX_test = scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8.Imbalanced Data Handling:"},{"metadata":{},"cell_type":"markdown","source":"# Before proceeding we have know that the data is imbalance. \nHandling class imbalance.\n\n1. Standard classifier algorithms like Decision Tree and Logistic Regression have a bias towards classes which have number of instances.\n\n2. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class.\n\n### Informed Over Sampling: Synthetic Minority Over-sampling Technique\n\nThis technique is followed to avoid overfitting which occurs when exact replicas of minority instances are added to the main dataset. A subset of data is taken from the minority class as an example and then new synthetic similar instances are created. These synthetic instances are then added to the original dataset. The new dataset is used as a sample to train the classification models.\n\n### Advantages\n\n1. Mitigates the problem of overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances\n2. No loss of useful information"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the sample before for 0 and 1 :\nprint(\"Before sample the counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before sample counts of label '0': {} \\n\".format(sum(y_train==0)))\nprint(\"Before sample churn event rate : {}% \\n\".format(round(sum(y_train==1)/len(y_train)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import the SMOTE and Joblib for solving this issue of imbalance. \nfrom sklearn.externals import joblib\nfrom imblearn.over_sampling import SMOTE\nfrom scipy import sparse\n\n\nsm = SMOTE(random_state=12, ratio = 1)\nX_train_re, y_train_re = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the sample after for 0 and 1 :\nprint(\"After sample the counts of label '1': {}\".format(sum(y_train_re==1)))\nprint(\"After sample counts of label '0': {} \\n\".format(sum(y_train_re==0)))\nprint(\"After sample churn event rate : {}% \\n\".format(round(sum(y_train_re==1)/len(y_train_re)*100,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now we can see that the data is now 50% balanced. Let us also do the outlier analysis and proceed further.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Outlier Analysis:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the outlier present in the data before doing modelling and remove the outliers and proceed with the modelling.\n\nprev_box = list(Highvalu_users.columns)\nfor i in Highvalu_users[prev_box]:\n    plt.figure(1,figsize=(15,5))\n    sns.boxplot(Highvalu_users[i])\n    plt.xticks(rotation = 90,fontsize =10)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Inferences:\nWe could see almost all the variables are having the ouliers preset in the datasets. Let us need to remove it."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Removal of the data sets from the outliers.Let us take the lower and upper limits are 0.05 and .9 on the upper. so that we could avoid loosing more data.\ntest_box_Df2 = list(Highvalu_users.columns) \nnew_copy = Highvalu_users[test_box_Df2]\nfor i in new_copy.columns:\n    Q1 = new_copy[i].quantile(0.05)\n    Q3 = new_copy[i].quantile(0.90)\n\n    IQR = Q3 - Q1\n    \n    lower_fence = Q1 - 1.5*IQR\n    upper_fence = Q3 + 1.5*IQR\n\n    new_copy[i][new_copy[i] <= lower_fence] = lower_fence\n    new_copy[i][new_copy[i] >= upper_fence] = upper_fence\n    \n    print(\"oUTLIERS:\",i,lower_fence,upper_fence)\n    \n    plt.figure(1,figsize=(10,5))\n    sns.boxplot(new_copy[i])\n    plt.xticks(rotation =90,fontsize =10)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Inferences : We have cleaned the data and now we can proceed for the modelling which we defined as we took lower ranges of the quartiles due to avoid loss of data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We have stored the cleaned data in newcopy. Let us check the head again\nnew_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the data shape again before modelling.\nnew_copy.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. PCA "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us do the PCA to reduce the dimentionality of the data and proceed with the Logistic regression first and move forward to highermodels \n\n#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# apply the PCA\npca.fit(X_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#List of PCA components.It would be the same as the number of variables\npca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Let's check the variance ratios\npca.explained_variance_ratio_[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import the matplot and visualse the pca variance ratio in a bar graph.\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (6,5))        # Size of the figure\n\nplt.bar(range(1,len(pca.explained_variance_ratio_[:50])+1), pca.explained_variance_ratio_[:50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# most of the data in the 0 to 2 ranges.  let us see the cummulative vairance ratio.\nvar_cumu = np.cumsum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see 0 to 10 most of the data are lying. \n# Make the scree plots cleary for choosing the no. of PCA \nplt.figure(figsize=(8,6))\nplt.title('Scree plots')\nplt.xlabel('No. of Components')\nplt.ylabel('Cummulative explained variance')\n\nplt.plot(range(1,len(var_cumu)+1), var_cumu)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Looks. we will take 35 components for desctribe the 95% of the varaince in the datasets.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# let us fit the data\nX_train_pca = pca_final.fit_transform(X_train_re)\nX_train_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(X_train_pca.transpose())\n# 1s ----> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Seems there is no correlation int the data sets between any two variables in PCA. We dont have any multicolinearity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Applying selected components to the test data - 35 components\nX_test_pca = pca_final.transform(X_test)\nX_test_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the prediction of churn customers we will be fitting variety of models and select one which is the best predictor of churn. Models trained are,\n\n1. Logistic Regression\n2. Decision Tree\n3. Random Forest\n4. Boosting models - Gradient Boosting Classifier and XGBoost Classifier\n5. SVM"},{"metadata":{},"cell_type":"markdown","source":"## 10. Logistic Regression &PCA:"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Import needed libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlr = LogisticRegression(class_weight='balanced')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Fit the algorithm on the data\ndef model_fit(alg, X_train, y_train, performCV=True, cv_folds=5):\n    alg.fit(X_train, y_train)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(X_train)\n    dtrain_predprob = alg.predict_proba(X_train)[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = cross_val_score(alg, X_train, y_train, cv=cv_folds, scoring='roc_auc')\n    \n    #Print model report:\n    print (\"\\nModel Summary:\")\n    print (\"Accuracy : %.4g\" % metrics.roc_auc_score(y_train, dtrain_predictions))\n    print (\"Recall/Sensitivity : %.4g\" % metrics.recall_score(y_train, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, dtrain_predprob))\n    \n    if performCV:\n        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us fit the model. \nmodel_fit(lr, X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Define the Modelmetrics .\ndef Model_metrics(Actual_churn=False,Predict_churn=False):\n\n    confusion = metrics.confusion_matrix(Actual_churn, Predict_churn)\n\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n\n    print(\"Roc_auc_score : {}\".format(metrics.roc_auc_score(Actual_churn, Predict_churn)))\n    # Let's see the sensitivity of our logistic regression model\n    print('Sensitivity/Recall : {}'.format(TP / float(TP+FN)))\n    # Let us calculate specificity\n    print('Specificity: {}'.format(TN / float(TN+FP)))\n    # Calculate false postive rate - predicting churn when customer does not have churned\n    print('False Positive Rate: {}'.format(FP/ float(TN+FP)))\n    # positive predictive value \n    print('Positive predictive value: {}'.format(TP / float(TP+FP)))\n    # Negative predictive value\n    print('Negative Predictive value: {}'.format(TN / float(TN+ FN)))\n    # sklearn precision score value \n    print('sklearn precision score value: {}'.format(metrics.precision_score(Actual_churn, Predict_churn)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predictions on Test data\npred_probs_test = lr.predict(X_test_pca)\nModel_metrics(y_test,pred_probs_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the accuracy, recall and precision again,\nprint(\"Accuracy : {}\".format(metrics.accuracy_score(y_test,pred_probs_test)))\nprint(\"Recall : {}\".format(metrics.recall_score(y_test,pred_probs_test)))\nprint(\"Precision : {}\".format(metrics.precision_score(y_test,pred_probs_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Making prediction on the test data\npred_probs_train = lr.predict_proba(X_train_pca)[:,1]\nprint(\"roc_auc_score(Train) {:2.2}\".format(metrics.roc_auc_score(y_train_re, pred_probs_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us define for the cutoff values\n#Function to find the optimal cutoff for classifing as churn/non-churn\n# Let's create columns with different probability cutoffs \ndef Optimal_Cutoff(df):\n    #Function to find the optimal cutoff for classifing as churn/non-churn\n    # Let's create columns with different probability cutoffs \n    numbers = [float(x)/10 for x in range(10)]\n    for i in numbers:\n        df[i] = df.churn_Prob.map( lambda x: 1 if x > i else 0)\n    #print(df.head())\n    \n    # Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n     # TP = confusion[1,1] # true positive \n    # TN = confusion[0,0] # true negatives\n    # FP = confusion[0,1] # false positives\n    # FN = confusion[1,0] # false negatives\n    \n    cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n    \n    from sklearn.metrics import confusion_matrix\n    \n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        cm1 = metrics.confusion_matrix(df.churn, df[i] )\n        total1=sum(sum(cm1))\n        accuracy = (cm1[0,0]+cm1[1,1])/total1\n        \n        speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n        sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n        cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    print(cutoff_df)\n    \n    # Let's plot accuracy sensitivity and specificity for various probabilities.\n    cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def predictChurnWithProb(model,X,y,prob):\n    # Funtion to predict the churn using the input probability cut-off\n    # Input arguments: model instance, x and y to predict using model and cut-off probability\n    \n    # predict\n    pred_probs = model.predict_proba(X)[:,1]\n    \n    y_df= pd.DataFrame({'churn':y, 'churn_Prob':pred_probs})\n    # Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\n    y_df['final_predicted'] = y_df.churn_Prob.map( lambda x: 1 if x > prob else 0)\n    # Let's see the head\n    Model_metrics(y_df.churn,y_df.final_predicted)\n    return y_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cut_off_prob=0.5\ny_train_df = predictChurnWithProb(lr,X_train_pca,y_train_re,cut_off_prob)\ny_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Let Draw the ROC curve:\n\nIt shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n\nThe closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n\nThe closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us define the roc curveL\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the ROC curve\ndraw_roc(y_train_df.churn, y_train_df.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# the ROC curve should be on the top left. showing good fit.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n#draw_roc(y_pred_final.Churn, y_pred_final.predicted)\nprint(\"roc_auc_score : {:2.2f}\".format(metrics.roc_auc_score(y_train_df.churn, y_train_df.final_predicted)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding Optimal Cutoff Point:\nA trade off between sensitivity(or recall) and specificity is to be considered in doing so.We need to tune the probability to get a better tradeoff for sensitivity and specificity\n# finding cut-off with the right balance of the metrices\n# sensitivity vs specificity trade-off\nfindOptimalCutoff(y_train_df)"},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# finding cut-off with the right balance of the metrices\n# sensitivity vs specificity trade-off\nOptimal_Cutoff(y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Note that the cutoff is between the 0.5 to 0.6. we choose to take the 0.52. at this point there will be balance of accuracy, sensitivity and specificity\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predicting with the choosen cut-off on train\ncut_off_prob = 0.52\nA = predictChurnWithProb(lr,X_train_pca,y_train_re,cut_off_prob)\nA.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us predict on the test set\n\n# predicting with the choosen cut-off on test\nB = predictChurnWithProb(lr,X_test_pca,y_test,cut_off_prob)\nB.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion on Logistic regression & PCA \n\n**A) Train Set :**\n\nRoc_auc_score : 0.82\n\nSensitivity/Recall : 0.83\n\nSpecificity: 0.82\n\n**B) for test set:**\n\nRoc_auc_score : 0.81\n\nSensitivity/Recall : 0.80\n\nSpecificity: 0.83\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 11. Decision Tree:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# let us apply the Decision tree with PCA and Hyperparameters and fit.\n\nDt = DecisionTreeClassifier(class_weight='balanced',\n                             max_features='auto',\n                             min_samples_split=100,\n                             min_samples_leaf=100,\n                             max_depth=6,\n                             random_state=10)\nmodel_fit(Dt, X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# make predictions\npred_probs_test = Dt.predict(X_test_pca)\n#Let's check the model metrices.\nModel_metrics(Actual_churn=y_test,Predict_churn=pred_probs_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# Create the parameter grid based on the results of random search \nparam_grid = {'max_depth': range(5,15,3),'min_samples_leaf': range(100, 400, 50),'min_samples_split': range(100, 400, 100),\n    'max_features': [8,10,15]}\n# Create a based model\ndt = DecisionTreeClassifier(class_weight='balanced',random_state=10)\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = dt, param_grid = param_grid, \n                          cv = 3, n_jobs = 4,verbose = 1,scoring=\"f1_weighted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fit the grid search to the data\ngrid_search.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# printing the optimal accuracy score and hyperparameters\nprint('Recall score',grid_search.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# model with the best hyperparameters\ndt_final = DecisionTreeClassifier(class_weight='balanced',\n                             max_depth=14,\n                             min_samples_leaf=100, \n                             min_samples_split=100,\n                             max_features=15,\n                             random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fit th model and get the model summary:\nmodel_fit(dt_final,X_train_pca,y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# make predictions in the test data\npred_probs_test = dt_final.predict(X_test_pca)\n#Let's check the model metrices.\nModel_metrics(Actual_churn=y_test,Predict_churn=pred_probs_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# classification report\nprint(classification_report(y_test,pred_probs_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# After tunning aslo, we can see we get 71% of the churn.\n# predicting churn with default cut-off 0.5\ncut_off_prob = 0.5\ny_train_df = predictChurnWithProb(dt_final,X_train_pca,y_train_re,cut_off_prob)\ny_train_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# finding cut-off with the right balance of the metrices\nOptimal_Cutoff(y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see that the cut off value is between 0.5 to 0.6. Let us chck again the cutoff as 0.52 and run again.\ncut_off_prob = 0.5\ny_train_df = predictChurnWithProb(dt_final,X_train_pca,y_train_re,cut_off_prob)\ny_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# At 0.52 we can see there is a balance in the sensitivity, specificity and acccuracy.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n#Lets see how it performs on test data.\ny_test_df= predictChurnWithProb(dt_final,X_test_pca,y_test,cut_off_prob)\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions:\n\n**A) Train data**\n\nAccuracy score : 0.83\n\nSensitivity/Recall : 0.83\n\nSpecificity: 0.83\n\n**B) Test data**\n\nAccuracy score : 0.76\n\nSensitivity/Recall : 0.70\n\nSpecificity: 0.82\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 12. Random Forest:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Again let us appy the Random forest pca and hypertuning with maxdepth\nparameters = {'max_depth': range(10, 30, 5)}\nrfc = RandomForestClassifier()\nrfgs = GridSearchCV(rfc, parameters,n_jobs=-1,\n                    cv=5, \n                   scoring=\"f1\")\nrfgs.fit(X_train_pca,y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores = rfgs.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see that the max_depth after 20th both the curves there were no significant change observed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us tune with the n_estimators \nparameters = {'n_estimators': range(50, 150, 25)}\nrf1 = RandomForestClassifier(max_depth=20,n_jobs=-1,random_state=10)\nrfgs = GridSearchCV(rf1, parameters, \n                    cv=3, \n                   scoring=\"recall\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# AGain fit the parameters\n\nrfgs.fit(X_train_pca,y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plotting accuracies with max_depth\ndef plt_traintest_acc(score,param):\n    scores = score\n    plt.figure()\n    plt.plot(scores[\"param_\"+param], \n    scores[\"mean_train_score\"], \n    label=\"training accuracy\")\n    plt.plot(scores[\"param_\"+param], \n    scores[\"mean_test_score\"], \n    label=\"test accuracy\")\n    plt.xlabel(param)\n    plt.ylabel(\"f1\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the plot\nplt_traintest_acc(rfgs.cv_results_,'n_estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see between 70 to 80, let us take 80 as our n_estimators ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Max features tuning with 80 \nparameters = {'max_features': [4, 8, 14, 20, 24]}\nrf3 = RandomForestClassifier(max_depth=20,n_estimators=80,n_jobs=-1,random_state=10)\nrfgs = GridSearchCV(rf3, parameters,cv=5,scoring=\"f1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us fit again and check \nrfgs.fit(X_train_pca,y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the acc. max features\nplt_traintest_acc(rfgs.cv_results_,'max_features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# As we see that at the 7.5 and later it is declining. \n# Let us tune the min sample leaf\nparameters = {'min_samples_leaf': range(100, 400, 50)}\nrf4 = RandomForestClassifier(max_depth=20,n_estimators=80,max_features=8,n_jobs=-1,random_state=10)\nrfgs = GridSearchCV(rf4, parameters,cv=3, scoring=\"f1\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us fit again and plt the curve \nrfgs.fit(X_train_pca,y_train_re)\nplt_traintest_acc(rfgs.cv_results_,'min_samples_leaf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# As we see that the Min samples leaf is 100.\n# let us tune min samples split.\nparameters = {'min_samples_split': range(50, 300, 50)}\nrf5 = RandomForestClassifier(max_depth=20,n_estimators=80,max_features=8,n_jobs=-1,min_samples_leaf=100,random_state=10)\nrfgs = GridSearchCV(rf5, parameters,cv=3,scoring=\"f1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Let us fit again and plt the curve \nrfgs.fit(X_train_pca,y_train_re)\nplt_traintest_acc(rfgs.cv_results_,'min_samples_split')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# the min samples split is 50 later almost flat and it is declining at 200.\n# Final - tunned in all aspects \n\nrf_final = RandomForestClassifier(max_depth=20,n_estimators=80,n_jobs=-1,max_features=8,min_samples_leaf=100,min_samples_split=50,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the train set.\nmodel_fit(rf_final,X_train_pca,y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict on test data\npredictions = rf_final.predict(X_test_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check the model metrics in test predictions\nModel_metrics(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# After fine tunning we can see the recall is for the final Random forest is only 73% with 80% accuracy.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the cutoff optimal value as we did for the other models\n# predicting churn with default cut-off 0.5\ncut_off_prob=0.5\ny_train_df = predictChurnWithProb(rf_final,X_train_pca,y_train_re,cut_off_prob)\ny_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us see the cut-off with the metrics of accuracy, sensitvity and specificity\nOptimal_Cutoff(y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see the optimal value is lying in 0.5 to 0.6. let us take as 0.52\ncut_off_prob=0.52\nA = predictChurnWithProb(rf_final,X_train_pca,y_train_re,cut_off_prob)\nA.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Make the prediction on the test:\ny_test_df= predictChurnWithProb(rf_final,X_test_pca,y_test,cut_off_prob)\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions:\nRandom Forest:\n\n**A) Train set:**\n\nRoc_auc_score : 0.85\n\nSensitivity/Recall : 0.84\n\nSpecificity: 0.87\n\n**B) Test set:**\n\nRoc_auc_score : 0.79\n\nSensitivity/Recall : 0.71\n\nSpecificity: 0.86"},{"metadata":{},"cell_type":"markdown","source":"## 13.Boosting Models:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Gradient boosing classifier with PCA and hypertuning \n# Impor the needed libraries\nfrom sklearn.ensemble import GradientBoostingClassifier  \n\n# Fitting the default GradientBoostingClassifier\nGb = GradientBoostingClassifier(random_state=10)\nmodel_fit(Gb, X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us tune the n_estimators\nparam_test_1 = {'n_estimators':range(20,150,10)}\ngsearch_1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \nparam_grid = param_test_1, scoring='f1',n_jobs=4,iid=False, cv=3)\ngsearch_1.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us check the best n_estimators first\nprint('gsearch_1.best_params_\\ngsearch1.best_score_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the score and n_estimators\nprint(\"The best n_estimators: {}\".format(gsearch_1.best_params_))\nprint(\"The best score: {}\".format(gsearch_1.best_score_))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us use the n_estimators =140 and tune the hyperparameters of max depth min samples spli & Learning rate is taken as 0.1.\nparam_test_2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\ngsearch_2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=140, max_features='sqrt', subsample=0.8, random_state=10), \nparam_grid = param_test_2, scoring='f1',n_jobs=4,iid=False, cv=3)\ngsearch_2.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the score and hyper tune parameters\nprint(\"The best max depth & min samples split: {}\".format(gsearch_2.best_params_))\nprint(\"The best score: {}\".format(gsearch_2.best_score_))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us tune the hyperparameters of  min samples leaf.\nparam_test_3 = {'min_samples_leaf':range(30,71,10)}\ngsearch_3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1,n_estimators=140,max_depth = 15,min_samples_split =200, max_features='sqrt', subsample=0.8, random_state=10), \nparam_grid = param_test_3, scoring='f1',n_jobs=4,iid=False, cv=3)\ngsearch_3.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check again the score and hyper tune parameter min_samples_leaf\nprint(\"The best min_samples_leaf: {}\".format(gsearch_3.best_params_))\nprint(\"The best score: {}\".format(gsearch_3.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check again the Max_features hypertuning and do the gridsearch.\nparam_test_4 = {'max_features':range(7,20,2)}\ngsearch_4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1,n_estimators=140,max_depth=15, min_samples_split=200, min_samples_leaf=30, subsample=0.8, random_state=10),\nparam_grid = param_test_4, scoring='f1',n_jobs=4,iid=False, cv=3)\ngsearch_4.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check again the score and hyper tune parameter \nprint(\"The best max_features: {}\".format(gsearch_4.best_params_))\nprint(\"The best score: {}\".format(gsearch_4.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# The Final Model for Gradient boosting with max_features =19 \nGb_final = GradientBoostingClassifier(learning_rate=0.1,n_estimators=140,max_features=19,max_depth=15, min_samples_split=200, min_samples_leaf=40, subsample=0.8, random_state=10)\nmodel_fit(Gb_final, X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predictions on Test data & check the metrics on test data\ntest_predict = Gb_final.predict(X_test_pca)\nModel_metrics(y_test,test_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us do the predict the churn for the default cutoff and fine tune later.\ncut_off_prob=0.5\ny_train_df = predictChurnWithProb(Gb_final,X_train_pca,y_train_re,cut_off_prob)\ny_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# let see the optimal cutoff:\nOptimal_Cutoff(y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see that the optimal cutoff points between 0.2 to 0.3. let us take as 0.2 and proceed.\ncut_off_prob=0.2\nA = predictChurnWithProb(Gb_final,X_train_pca,y_train_re,cut_off_prob)\nA.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us do predict in the test data:\ny_test_df= predictChurnWithProb(Gb_final,X_test_pca,y_test,cut_off_prob)\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions:\n\nThe Model in the training data is completely overfitting as we see sensitivity and accuracy is 0.1 as we see it has lower performance in the test set.\n\n**A)Train set**\n\nRoc_auc_score : 0.99\n\nSensitivity/Recall : 1.0\n\nSpecificity: 0.98\n\n**B)Test set**\n\nRoc_auc_score : 0.79\n\nSensitivity/Recall : 0.69\n\nSpecificity: 0.88"},{"metadata":{},"cell_type":"markdown","source":"## Let us do with the Xgboosting:"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import the needed libraries\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n# Fitting the XGBClassifier\nXGb = XGBClassifier(learning_rate =0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,colsample_bytree=0.8,\n                    objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=27)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model fit and performance on Train data\nmodel_fit(XGb, X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us tune the hyperparameters one by one\nparam_test_1 = {'max_depth':range(3,10,2),'min_child_weight':range(1,6,2)}\ngsearch_1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n             min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n            param_grid = param_test_1, scoring='f1',n_jobs=4,iid=False, cv=3)\ngsearch_1.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check again the score and hyper tune parameter \nprint(\"The best ma_depth & min_child_weight: {}\".format(gsearch_1.best_params_))\nprint(\"The best score: {}\".format(gsearch_1.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us tune the hyperparameters one by one\nparam_test_2 = {'gamma':[i/10.0 for i in range(0,5)]}\ngsearch_2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=9,min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n             objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test_2, scoring='f1',n_jobs=4,iid=False, cv=3)\ngsearch_2.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check again the score and hyper tune parameter \nprint(\"The best gamma: {}\".format(gsearch_2.best_params_))\nprint(\"The best score: {}\".format(gsearch_2.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Final XGb boosing with gamma 0.2 & fit the model.\nXGb = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=9,min_child_weight=1, gamma=0.2, subsample=0.8, colsample_bytree=0.8,\n     objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\nmodel_fit(XGb, X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predict on the test data and do the model metrics.\ntest_predict = XGb.predict(X_test_pca)\nModel_metrics(y_test,test_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Take the default cutoff and check again\n# predicting churn with default cut-off 0.5\ncut_off_prob=0.5\ny_train_df = predictChurnWithProb(XGb,X_train_pca,y_train_re,cut_off_prob)\ny_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Finding optimal cut-off probability\nOptimal_Cutoff(y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# the optimal cutoff point will be between 0.3 to 0.5, taken as 0.4\ncut_off_prob=0.4\nA = predictChurnWithProb(XGb,X_train_pca,y_train_re,cut_off_prob)\nA.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us predict in the test data\ny_test_df= predictChurnWithProb(XGb,X_test_pca,y_test,cut_off_prob)\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions:\n\n**A) Train set**\n\nRoc_auc_score : 0.99\n\nSensitivity/Recall : 0.99\n\nSpecificity: 0.98\n\n**B) Test set**\n\nRoc_auc_score : 0.77\n\nSensitivity/Recall : 0.63\n\nSpecificity: 0.92"},{"metadata":{},"cell_type":"markdown","source":"## 14. SVM :"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us try doing the model test in SVC and check. https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n# note that we are using cost C=1\nfrom sklearn.svm import SVC \n\nSv = SVC(C = 1)\n\n# fit\nSv.fit(X_train_pca, y_train_re)\n\n# predict on train\nS_predict = Sv.predict(X_train_pca)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the model metrics\nModel_metrics(y_train_re,S_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predict on test data\nS_predict = Sv.predict(X_test_pca)\nModel_metrics(y_test,S_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us do the hyper tunning \n# specify range of parameters (C) as a list\nparams = {\"C\": [0.1, 1, 10, 100, 1000]}\n\nSvm= SVC()\n\n# set up grid search and fit on the train set.\n\nmodel_cv = GridSearchCV(estimator = Svm, param_grid = params, scoring= 'f1',cv = 5, verbose = 1,n_jobs=4,return_train_score=True) \nmodel_cv.fit(X_train_pca, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check again the score and hyper tune parameter \nprint(\"The best params: {}\".format(model_cv.best_params_))\nprint(\"The best score: {}\".format(model_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# use the C as 1000 and run again\nSv_final = SVC(C = 1000)\n\n# fit\nSv_final.fit(X_train_pca, y_train_re)\n\n# predict on train\nS_predict = Sv_final.predict(X_train_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the model metrics\nModel_metrics(y_train_re,S_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check in the test set\nS_predict = Sv_final.predict(X_test_pca)\nModel_metrics(y_test,S_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions:\n\n**A) Train set:**\n\nRoc_auc_score : 0.85\n\nSensitivity/Recall : 0.84\n\nSpecificity: 0.86\n\n**B) Test set:**\n\nRoc_auc_score : 0.81\n\nSensitivity/Recall : 0.75\n\nSpecificity: 0.86"},{"metadata":{},"cell_type":"markdown","source":"## 15. Conclusions:\n\nWe have used the data set in all the Models. The best choice will be selected based on the test data Recall. \n\nEspecailly for this chrun case , the Recall / Sensitivity is most important and telecom industry must work on these customers to retain rather than acquiring new customers. \n\nAs it will more costly in acquiring due to convience the customers, advertisements etc., so better to retain the customers and improve the services on the varaibles to avoid risk which will decline the revenue of the company..\n\nSummary of the Models:\n\n1. Logistic Regression with PCA:\n\n\nA) Train Set :\nRoc_auc_score : 0.82\n**Sensitivity/Recall : 0.83**\nSpecificity: 0.82\n\nB) Test set:\nRoc_auc_score : 0.81\n**Sensitivity/Recall : 0.80**\nSpecificity: 0.83\n\n2. Decision Tree :\n\nA) Train data\nAccuracy score : 0.83\n**Sensitivity/Recall : 0.83**\nSpecificity: 0.83\n\nB) Test data\nAccuracy score : 0.76\n**Sensitivity/Recall : 0.70**\nSpecificity: 0.82\n\n3. Random Forest :\n\nA) Train set:\nRoc_auc_score : 0.85\n**Sensitivity/Recall : 0.84**\nSpecificity: 0.87\n\nB) Test set:\nRoc_auc_score : 0.79\n**Sensitivity/Recall : 0.71**\nSpecificity: 0.86\n\n4. Boosting Models:\n\nA) Train set\nRoc_auc_score : 0.99\n**Sensitivity/Recall : 0.99**\nSpecificity: 0.98\n\nB) Test set\nRoc_auc_score : 0.77\n**Sensitivity/Recall : 0.63**\nSpecificity: 0.92\n\n5. SVC :\n\nA) Train set:\nRoc_auc_score : 0.85\n**Sensitivity/Recall : 0.84**\nSpecificity: 0.86\n\nB) Test set:\nRoc_auc_score : 0.81\n**Sensitivity/Recall : 0.75**\nSpecificity: 0.86\n\n\nWe Suggest to see that the Recall/ Senstivity for the Logistic regression wiht PCA is good for the prediction as is 0.80 compared to other models. WE can see that SVC is ok as we got 0.75 recall. Boosting models are overfitting the data."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let us see the top variables which contributes more to the chrun rather than false postives.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Random forest is used to derive the churn\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [8,10,12],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [12, 15, 20]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = 4,verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fit the grid search to the data\ngrid_search.fit(X_train_re, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# printing the optimal accuracy score and hyperparameters\nprint('Accuracy is',grid_search.best_score_,'using',grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Apply rf calssifier with hyper tuning and fit the model\nrf = RandomForestClassifier(max_depth=12,max_features=20,min_samples_leaf=100,min_samples_split=200,n_estimators=300,random_state=10)\nrf.fit(X_train_re, y_train_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot the bar graph\nplt.figure(figsize=(10,40))\nfeat_importances = pd.Series(rf.feature_importances_, index=X.columns)\nfeat_importances.nlargest(len(X.columns)).sort_values().plot(kind='barh', align='center')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that the top 25 variables the telecom company must concentrate.\n\nInorder to improve the service they need to really work on these variables to be more competitive in the market which will help them to improve the revenue of the orgaisation. \n\nAlso, where there is always there is decrease in these varibles will drastically affect the revenue and the customer will be lost if we dont take necessary steps. \n\nDo launch programs to retain the customer rather than acquiring the new customer. as it will more costly to acquire the new customers rather than retianing the old customers."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}